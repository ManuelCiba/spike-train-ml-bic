# Use the results (connectivity matrices) generated by script "main.py"
# and scale the matrices to values between -1 and 1 using a common scaler
# for control and bicuculline group


import numpy as np
from sklearn.preprocessing import StandardScaler
import os
import pandas as pd
import glob
import warnings

from lib.connectivity import matrix_handling
from lib.data_handler import folder_structure
import settings

# GLOBAL VARS
SOURCE_DATA_FOLDER = settings.FOLDER_NAME_matrices_raw
TARGET_DATA_FOLDER = settings.FOLDER_NAME_matrices_scaled

def _scale_matrix(M, M_list):

    # M: matrix that has to be scaled
    # M_list: all matrices (of BIC00 and BIC10), used to define the scaler

    # Set the diagonal elements to np.nan
    np.fill_diagonal(M, np.nan)
    M[M == 0] = np.nan
    for m in M_list:
        np.fill_diagonal(m, np.nan)
        m[m == 0] = np.nan

    # Combine the data from all matrices (bic00 and bic10)
    M_combined = np.concatenate(M_list, axis=0)

    # Use the StandardScaler with global mean and standard deviation
    scaler = StandardScaler().fit(M_combined)

    # Transform the data for both conditions
    M_scaled = scaler.transform(M)  # will replace NaNs by mean value of scaler

    return M_scaled


def scale_and_save(chip_path):
    path_bic00 = os.path.join(chip_path, "bic00")
    path_bic10 = os.path.join(chip_path, "bic10")

    files_bic00 = sorted(glob.glob(path_bic00 + '/*.csv'))
    files_bic10 = sorted(glob.glob(path_bic10 + '/*.csv'))

    # laod all matrices and save into common list M_list
    M_list = []
    for file_bic00 in files_bic00:
        M_list.append(np.array(matrix_handling.load_correlation_matrix(file_bic00)))
    for file_bic10 in files_bic10:
        M_list.append(np.array(matrix_handling.load_correlation_matrix(file_bic10)))

    # scale bic00 group
    for file_bic00 in files_bic00:

        # only calculate if result does not exist yet
        full_path = file_bic00.replace(SOURCE_DATA_FOLDER, TARGET_DATA_FOLDER)
        if not os.path.isfile(full_path):
            M = np.array(matrix_handling.load_correlation_matrix(file_bic00))
            M_scaled = _scale_matrix(M, M_list)
            matrix_handling.save_correlation_matrix(M_scaled, full_path)
            matrix_handling.plot_correlation_matrix(M_scaled, full_path.replace("csv", "jpg"))
        else:
            print("Already calculated: " + full_path)

    # scale bic10 group
    for file_bic10 in files_bic10:

        # only calculate if result does not exist yet
        full_path = file_bic10.replace(SOURCE_DATA_FOLDER, TARGET_DATA_FOLDER)
        if not os.path.isfile(full_path):
            M = np.array(matrix_handling.load_correlation_matrix(file_bic10))
            M_scaled = _scale_matrix(M, M_list)
            matrix_handling.save_correlation_matrix(M_scaled, full_path)
            matrix_handling.plot_correlation_matrix(M_scaled, full_path.replace("csv", "jpg"))
        else:
            print("Already calculated: " + full_path)



if __name__ == '__main__':

    warnings.filterwarnings("ignore")

    base_folder = os.path.join(settings.PATH_RESULTS_FOLDER, SOURCE_DATA_FOLDER)
    chip_path_list = folder_structure.get_all_paths(base_folder, 'rec')

    for chip_path in chip_path_list:
        print("Scaling Chip: " + chip_path)
        scale_and_save(chip_path)

